{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_mining(synopsis):\n",
    "    tokens = nltk.word_tokenize(synopsis)                           #tokenization\n",
    "    tokens = [word for word in tokens if word.isalpha()]            #remove punctuations and numbers\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))        #remove stopwords\n",
    "    tokens = [word for word in tokens if not word.lower() in stop_words]       \n",
    "    tokens = [PorterStemmer().stem(word) for word in tokens]        #stemming\n",
    "    tokens = [str(hash(word)) for word in tokens]                   #consider the hash code of the words will be useful for the next queries\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(text):\n",
    "    for word in set(text):                                          #create the vocabuary using the synopsis \n",
    "        vocabulary.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = {}\n",
    "vocabulary = []\n",
    "num_pag = 20\n",
    "for j in range(0,num_pag):\n",
    "    for i in range((50*j)+1, (50*(j+1))+1):\n",
    "        file = open( \"pages_tsv/pages_tsv/page_\" + str(j+1) + \"/anime_\" + str(i)+ \".tsv\", \"r\", encoding = \"utf-8\")       \n",
    "        anime = file.read()                                             \n",
    "        file.close()\n",
    "        anime = anime.split(\"\\t\")\n",
    "        synopsis = anime[24]\n",
    "        synopsis = text_mining(synopsis)\n",
    "        vocab(synopsis)\n",
    "        doc = \"\".join(\"documento \"+str(i))\n",
    "        description[doc] = synopsis\n",
    "        title = anime[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set(vocabulary))                    #create the vocabulatory with non repetitions of words\n",
    "file = open(\"vocabulary.txt\", \"w\", encoding = \"utf-8\")\n",
    "for word in vocabulary:\n",
    "    file.write(str(hash(word)) +\"\\n\")                 #create the file .txt that maps each word to a term_id using tha hash\n",
    "file.close()                                          #function of python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "for term_id in vocabulary:\n",
    "    for i in description:\n",
    "        if term_id in description[i]:\n",
    "            if term_id not in inverted_index:\n",
    "                inverted_index[term_id] =  [i]\n",
    "            else:\n",
    "                inverted_index[term_id] = inverted_index[term_id] + [i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word(query):\n",
    "    d = []\n",
    "    query = set(text_mining(query))\n",
    "    for word in query:\n",
    "        d.append(inverted_index[word])\n",
    "    print(d)\n",
    "    for i in range(len(d)-1):\n",
    "        s = set(d[i]).intersection(set(d[i+1]))\n",
    "    return  s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['documento 91', 'documento 100', 'documento 119', 'documento 141', 'documento 143', 'documento 147', 'documento 173', 'documento 184', 'documento 235', 'documento 236', 'documento 240', 'documento 260', 'documento 271', 'documento 297', 'documento 314', 'documento 333', 'documento 365', 'documento 371', 'documento 378', 'documento 391', 'documento 397', 'documento 401', 'documento 452', 'documento 457', 'documento 494', 'documento 501', 'documento 504', 'documento 515', 'documento 518', 'documento 529', 'documento 560', 'documento 603', 'documento 608', 'documento 613', 'documento 661', 'documento 680', 'documento 694', 'documento 726', 'documento 732', 'documento 749', 'documento 750', 'documento 774', 'documento 798', 'documento 802', 'documento 811', 'documento 831', 'documento 898', 'documento 910', 'documento 928'], ['documento 365', 'documento 401']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'documento 365', 'documento 401'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"saiyan race\"\n",
    "search_word(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
